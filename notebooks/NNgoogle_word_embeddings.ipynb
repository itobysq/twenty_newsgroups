{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN Google word vectors\n",
    "Build a basic nueral net using the google word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../scripts')\n",
    "import classifier_helpers as ch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "         self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "         return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n",
    "                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    plt.ylim(ylim)\n",
    "    plt.xlabel('Training examples')\n",
    "    plt.ylabel('Score')\n",
    "    train_sizes, train_scores, test_scores = learning_curve(estimator, \n",
    "                                                            X, \n",
    "                                                            y, \n",
    "                                                            cv=cv, \n",
    "                                                            n_jobs=n_jobs, \n",
    "                                                            train_sizes=train_sizes)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['rec.motorcycles', 'rec.autos']\n",
    "newsgroups_train = fetch_20newsgroups(subset='train',\n",
    "                                      categories=categories,\n",
    "                                      remove=('headers', 'footers', 'quotes'))\n",
    "newsgroups_test = fetch_20newsgroups(subset='test',\n",
    "                                     remove=('headers', 'footers', 'quotes'),\n",
    "                                      categories=categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv = gensim.models.KeyedVectors.load_word2vec_format('../data/GoogleNews-vectors-negative300.bin.gz',\n",
    "                                                    binary=True, limit=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wave'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.doesnt_match(['wheel', 'car', 'wave'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = dict(zip(wv.index2word, wv.vectors))\n",
    "vectors = []\n",
    "for email in newsgroups_train.data:\n",
    "    email_vector = ch.average_vectors(email, w2v, vec_length=300)\n",
    "    vectors.append(email_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1192"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1192, 300)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors = np.vstack(vectors)\n",
    "vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " ...\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    var_input = tf.placeholder(dtype=tf.float32, shape=[None, 300], name='var_input')\n",
    "    b = tf.zeros([2])\n",
    "    W = tf.zeros([300, 2])\n",
    "    product = tf.matmul(var_input, W) + b\n",
    "    result = sess.run(product, feed_dict={var_input:vectors})\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "isess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_input = tf.placeholder(dtype=tf.float32, shape=[None, 300], name='var_input')\n",
    "b = tf.zeros([2])\n",
    "W = tf.zeros([300, 2])\n",
    "product = tf.matmul(var_input, W) + b\n",
    "isess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       ...,\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product.eval(feed_dict={var_input:vectors})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some placeholder variables\n",
    "x_ = tf.placeholder(tf.float32, shape=[None, 300], name='input')\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 2], name='output')\n",
    "\n",
    "# Define the network computation\n",
    "W = tf.Variable(tf.zeros([300, 2]))\n",
    "b = tf.Variable(tf.zeros([2]))\n",
    "yhat = tf.nn.softmax(tf.matmul(x_, W) + b)\n",
    "\n",
    "# Define our loss function\n",
    "mse_loss = tf.reduce_mean(tf.square(yhat - y_))\n",
    "\n",
    "# Compute accuracy computation\n",
    "correct_prediction = tf.equal(tf.argmax(yhat,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# Set up the training method\n",
    "train_step = tf.train.AdamOptimizer(0.1).minimize(mse_loss)\n",
    "isess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       ...,\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       [1, 0]])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(vectors, \n",
    "                                                     newsgroups_train.target, \n",
    "                                                     test_size=0.2, \n",
    "                                                     random_state=42)\n",
    "\n",
    "ytr = np.vstack(((y_train), (1-y_train))).T\n",
    "yte = np.vstack(((y_test), (1-y_test))).T\n",
    "ytr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1, 1])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5057712"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy.eval(feed_dict={x_: X_train, y_: ytr})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False,  True, False,  True,  True, False, False,\n",
       "       False, False, False, False,  True, False,  True, False,  True,\n",
       "        True,  True, False, False, False,  True,  True,  True,  True,\n",
       "       False,  True, False, False,  True, False, False,  True, False,\n",
       "       False,  True,  True, False, False, False,  True, False, False,\n",
       "       False,  True,  True, False, False, False,  True, False, False,\n",
       "        True, False, False, False, False,  True,  True,  True, False,\n",
       "        True, False,  True, False,  True, False, False, False, False,\n",
       "        True, False,  True, False,  True, False,  True, False, False,\n",
       "       False,  True,  True,  True, False,  True, False,  True, False,\n",
       "       False,  True,  True, False, False,  True,  True,  True, False,\n",
       "        True,  True, False,  True,  True,  True, False,  True,  True,\n",
       "       False,  True,  True,  True,  True,  True, False, False, False,\n",
       "       False, False, False,  True, False,  True, False,  True,  True,\n",
       "       False,  True, False, False, False, False, False, False, False,\n",
       "       False,  True,  True,  True,  True,  True, False,  True,  True,\n",
       "        True, False,  True,  True,  True, False, False,  True, False,\n",
       "        True, False,  True,  True, False,  True, False,  True, False,\n",
       "        True,  True,  True, False, False, False, False,  True,  True,\n",
       "       False,  True, False, False,  True, False,  True,  True, False,\n",
       "        True,  True, False, False,  True, False,  True,  True, False,\n",
       "        True,  True, False,  True,  True, False, False,  True,  True,\n",
       "        True,  True, False,  True,  True,  True,  True, False, False,\n",
       "        True, False,  True,  True, False,  True, False, False,  True,\n",
       "        True,  True, False,  True, False, False, False, False,  True,\n",
       "       False, False, False,  True,  True, False,  True,  True, False,\n",
       "       False, False,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True, False, False, False, False, False, False,  True,  True,\n",
       "        True,  True, False,  True,  True,  True,  True,  True, False,\n",
       "        True,  True,  True, False, False, False, False, False,  True,\n",
       "        True, False, False, False, False, False, False, False,  True,\n",
       "        True,  True, False, False,  True,  True,  True, False,  True,\n",
       "       False,  True,  True,  True, False, False,  True, False, False,\n",
       "        True,  True,  True, False, False, False,  True, False, False,\n",
       "        True, False, False, False, False, False,  True,  True, False,\n",
       "       False, False, False,  True, False,  True,  True,  True,  True,\n",
       "        True, False, False, False, False,  True,  True, False,  True,\n",
       "       False,  True, False,  True,  True, False, False, False, False,\n",
       "       False,  True, False, False,  True, False, False,  True, False,\n",
       "       False,  True,  True, False,  True, False, False,  True, False,\n",
       "        True, False, False, False,  True,  True, False, False, False,\n",
       "        True,  True,  True,  True,  True,  True,  True, False, False,\n",
       "       False, False,  True,  True,  True, False, False,  True, False,\n",
       "       False, False,  True,  True, False, False,  True, False,  True,\n",
       "        True, False, False, False,  True,  True, False, False,  True,\n",
       "        True, False, False, False,  True, False, False,  True,  True,\n",
       "       False, False,  True,  True,  True, False,  True, False,  True,\n",
       "       False, False,  True,  True, False, False,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True, False,  True, False,  True,  True,  True,  True,\n",
       "        True,  True, False, False, False, False,  True, False,  True,\n",
       "       False, False,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True, False, False, False,  True,  True, False,\n",
       "        True,  True,  True,  True,  True,  True, False, False,  True,\n",
       "       False, False, False,  True,  True, False,  True, False,  True,\n",
       "       False, False,  True, False, False,  True, False,  True, False,\n",
       "       False, False, False,  True,  True,  True,  True,  True,  True,\n",
       "       False,  True, False, False, False,  True,  True, False, False,\n",
       "       False,  True, False,  True, False, False, False,  True,  True,\n",
       "       False, False, False,  True, False,  True, False,  True,  True,\n",
       "        True,  True,  True, False,  True,  True, False, False,  True,\n",
       "        True, False,  True, False, False, False,  True, False,  True,\n",
       "        True,  True,  True,  True, False, False, False,  True, False,\n",
       "       False, False,  True, False, False,  True,  True, False, False,\n",
       "        True, False, False,  True,  True,  True, False,  True,  True,\n",
       "        True, False, False, False,  True,  True, False,  True,  True,\n",
       "       False, False, False,  True,  True, False,  True,  True,  True,\n",
       "        True,  True,  True,  True, False, False,  True, False, False,\n",
       "        True,  True, False, False, False,  True,  True, False, False,\n",
       "        True,  True, False, False, False, False, False,  True,  True,\n",
       "        True,  True, False,  True, False,  True,  True,  True, False,\n",
       "       False,  True, False, False, False,  True, False, False,  True,\n",
       "        True,  True,  True, False, False,  True, False,  True,  True,\n",
       "       False, False,  True,  True,  True,  True,  True,  True, False,\n",
       "       False, False,  True, False,  True, False, False, False, False,\n",
       "        True,  True,  True,  True, False, False,  True, False, False,\n",
       "        True,  True, False, False,  True, False, False,  True,  True,\n",
       "        True,  True, False,  True, False, False,  True,  True, False,\n",
       "        True, False, False,  True,  True, False,  True,  True, False,\n",
       "        True,  True,  True, False,  True, False, False,  True,  True,\n",
       "       False,  True, False, False,  True,  True, False, False,  True,\n",
       "        True, False,  True,  True, False,  True,  True, False, False,\n",
       "        True, False, False, False,  True,  True, False, False,  True,\n",
       "        True, False, False,  True,  True, False, False, False,  True,\n",
       "        True, False,  True, False, False,  True, False,  True,  True,\n",
       "       False,  True, False, False,  True,  True, False,  True, False,\n",
       "        True, False, False,  True,  True, False, False, False,  True,\n",
       "        True, False,  True,  True, False, False, False, False, False,\n",
       "        True,  True,  True,  True, False,  True, False, False, False,\n",
       "        True,  True, False, False, False, False, False, False,  True,\n",
       "        True, False, False, False,  True, False,  True,  True,  True,\n",
       "        True,  True,  True, False, False,  True, False, False,  True,\n",
       "        True, False,  True,  True, False,  True, False,  True,  True,\n",
       "        True, False,  True, False, False,  True, False, False,  True,\n",
       "       False, False,  True,  True, False, False, False,  True,  True,\n",
       "        True,  True,  True,  True,  True, False,  True,  True, False,\n",
       "       False, False, False,  True,  True, False,  True, False,  True,\n",
       "       False, False, False,  True,  True,  True,  True,  True, False,\n",
       "        True, False, False,  True, False,  True, False,  True, False,\n",
       "       False, False, False, False,  True, False, False,  True,  True,\n",
       "        True, False, False, False, False,  True,  True, False,  True,\n",
       "       False, False, False, False,  True, False, False, False,  True,\n",
       "        True, False,  True, False,  True,  True, False,  True,  True,\n",
       "        True,  True,  True, False,  True, False,  True,  True, False,\n",
       "       False,  True, False, False, False,  True, False, False,  True,\n",
       "       False,  True,  True, False, False, False,  True,  True])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_prediction.eval(feed_dict={x_: X_train, y_: ytr})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train step: 0, Loss: 0.27712225914001465, Accuracy: 48.53556454181671\n",
      "Train step: 100, Loss: 0.22679166495800018, Accuracy: 64.43514823913574\n",
      "Train step: 200, Loss: 0.22031888365745544, Accuracy: 64.85355496406555\n",
      "Train step: 300, Loss: 0.21657274663448334, Accuracy: 66.10878705978394\n",
      "Train step: 400, Loss: 0.213670015335083, Accuracy: 66.10878705978394\n",
      "Train step: 500, Loss: 0.21140432357788086, Accuracy: 66.10878705978394\n",
      "Train step: 600, Loss: 0.20961931347846985, Accuracy: 66.52719378471375\n",
      "Train step: 700, Loss: 0.20813098549842834, Accuracy: 66.94560647010803\n",
      "Train step: 800, Loss: 0.20686796307563782, Accuracy: 67.78242588043213\n",
      "Train step: 900, Loss: 0.20582959055900574, Accuracy: 68.20083856582642\n",
      "Train step: 1000, Loss: 0.20495343208312988, Accuracy: 67.36401915550232\n",
      "Train step: 1100, Loss: 0.2041929066181183, Accuracy: 67.78242588043213\n",
      "Train step: 1200, Loss: 0.20361687242984772, Accuracy: 67.36401915550232\n",
      "Train step: 1300, Loss: 0.20298469066619873, Accuracy: 66.94560647010803\n",
      "Train step: 1400, Loss: 0.20248916745185852, Accuracy: 66.52719378471375\n",
      "Train step: 1500, Loss: 0.20204418897628784, Accuracy: 66.52719378471375\n"
     ]
    }
   ],
   "source": [
    "# Train the model!\n",
    "n_iters = 1500\n",
    "for i in range(n_iters+1):\n",
    "    # Run through an iteration of the training process\n",
    "    train_step.run(feed_dict={x_: X_train, y_: ytr})\n",
    "    \n",
    "    # Compute the accuracy and loss\n",
    "    if i % 100 == 0:\n",
    "        current_loss = mse_loss.eval(feed_dict={x_: X_train, y_: ytr})\n",
    "        current_acc  = accuracy.eval(feed_dict={x_: X_test, y_: yte})\n",
    "        print('Train step: {}, Loss: {}, Accuracy: {}'.format(i, \n",
    "                                                                current_loss, \n",
    "                                                                current_acc * 100.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train step: 0, Loss: 0.431, Accuracy: 50.577%\n",
      "Train step: 100, Loss: 0.281, Accuracy: 50.577%\n",
      "Train step: 200, Loss: 0.326, Accuracy: 50.577%\n",
      "Train step: 300, Loss: 0.289, Accuracy: 50.577%\n",
      "Train step: 400, Loss: 0.271, Accuracy: 50.577%\n",
      "Train step: 500, Loss: 0.261, Accuracy: 50.577%\n",
      "Train step: 600, Loss: 0.256, Accuracy: 50.577%\n",
      "Train step: 700, Loss: 0.253, Accuracy: 50.577%\n",
      "Train step: 800, Loss: 0.251, Accuracy: 50.577%\n",
      "Train step: 900, Loss: 0.250, Accuracy: 50.577%\n",
      "Train step: 1000, Loss: 0.250, Accuracy: 50.577%\n",
      "Train step: 1100, Loss: 0.250, Accuracy: 51.731%\n",
      "Train step: 1200, Loss: 0.250, Accuracy: 51.522%\n",
      "Train step: 1300, Loss: 0.249, Accuracy: 51.731%\n",
      "Train step: 1400, Loss: 0.249, Accuracy: 51.836%\n",
      "Train step: 1500, Loss: 0.249, Accuracy: 51.836%\n"
     ]
    }
   ],
   "source": [
    "# Now we are creating two weight matrices, one that contains the\n",
    "# weights connecting the input units to the hidden units, and one\n",
    "# connecting the hidden units to the output units\n",
    "n_inputs = 300\n",
    "n_hidden = 300\n",
    "n_outputs = 2\n",
    "W_input_to_hidden = tf.Variable(tf.truncated_normal([n_inputs, n_hidden], stddev=0.1))\n",
    "W_hidden_to_output = tf.Variable(tf.truncated_normal([n_hidden, n_outputs], stddev=0.1))\n",
    "b_hidden = tf.Variable(tf.constant(0.1, shape=[n_hidden]))\n",
    "b_output = tf.Variable(tf.constant(0.1, shape=[n_outputs]))\n",
    "\n",
    "# We now redefine the neural computation. I'm showing it here in\n",
    "# two steps: one for each layer in the network\n",
    "hidden_activation = tf.nn.sigmoid(tf.matmul(x_, W_input_to_hidden) + b_hidden)\n",
    "yhat = tf.nn.softmax(tf.matmul(hidden_activation, W_hidden_to_output) + b_output)\n",
    "\n",
    "############################\n",
    "# The rest is the same...\n",
    "mse_loss = tf.reduce_mean(tf.square(yhat - y_))\n",
    "correct_prediction = tf.equal(tf.argmax(yhat,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "train_step = tf.train.AdagradOptimizer(0.1).minimize(mse_loss)\n",
    "isess.run(tf.initialize_all_variables())\n",
    "n_iters = 1500\n",
    "for i in range(n_iters+1):\n",
    "    train_step.run(feed_dict={x_: X_train, y_: ytr})\n",
    "    if i % 100 == 0:\n",
    "        current_loss = mse_loss.eval(feed_dict={x_: X_train, y_: ytr})\n",
    "        current_acc  = accuracy.eval(feed_dict={x_: X_train, y_: ytr})\n",
    "        print('Train step: %d, Loss: %.3f, Accuracy: %.3f%%' % (i, \n",
    "                                                                current_loss, \n",
    "                                                                current_acc * 100.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
