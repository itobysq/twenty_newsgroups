{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN Google word vectors\n",
    "Build a basic nueral net using the google word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../scripts')\n",
    "import classifier_helpers as ch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "         self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "         return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n",
    "                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    plt.ylim(ylim)\n",
    "    plt.xlabel('Training examples')\n",
    "    plt.ylabel('Score')\n",
    "    train_sizes, train_scores, test_scores = learning_curve(estimator, \n",
    "                                                            X, \n",
    "                                                            y, \n",
    "                                                            cv=cv, \n",
    "                                                            n_jobs=n_jobs, \n",
    "                                                            train_sizes=train_sizes)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['rec.motorcycles', 'rec.autos']\n",
    "newsgroups_train = fetch_20newsgroups(subset='train',\n",
    "                                      categories=categories,\n",
    "                                      remove=('headers', 'footers', 'quotes'))\n",
    "newsgroups_test = fetch_20newsgroups(subset='test',\n",
    "                                     remove=('headers', 'footers', 'quotes'),\n",
    "                                      categories=categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.16326530612245"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(newsgroups_train.data) / 98"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "throwaway_data, data, throwaway_target, target = train_test_split(newsgroups_train.data,\n",
    "                                                                  newsgroups_train.target,\n",
    "                                                                  test_size = 1/12, \n",
    "                                                                  random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv = gensim.models.KeyedVectors.load_word2vec_format('../data/GoogleNews-vectors-negative300.bin.gz',\n",
    "                                                    binary=True, limit=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wave'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.doesnt_match(['wheel', 'car', 'wave'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = dict(zip(wv.index2word, wv.vectors))\n",
    "vectors = []\n",
    "for email in data:\n",
    "    email_vector = ch.average_vectors(email, w2v, vec_length=300)\n",
    "    vectors.append(email_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 300)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors = np.vstack(vectors)\n",
    "vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    var_input = tf.placeholder(dtype=tf.float32, shape=[None, 300], name='var_input')\n",
    "    b = tf.zeros([2])\n",
    "    W = tf.zeros([300, 2])\n",
    "    product = tf.matmul(var_input, W) + b\n",
    "    result = sess.run(product, feed_dict={var_input:vectors})\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "isess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_input = tf.placeholder(dtype=tf.float32, shape=[None, 300], name='var_input')\n",
    "b = tf.zeros([2])\n",
    "W = tf.zeros([300, 2])\n",
    "product = tf.matmul(var_input, W) + b\n",
    "isess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product.eval(feed_dict={var_input:vectors})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some placeholder variables\n",
    "x_ = tf.placeholder(tf.float32, shape=[None, 300], name='input')\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 2], name='output')\n",
    "\n",
    "# Define the network computation\n",
    "W = tf.Variable(tf.zeros([300, 2]))\n",
    "b = tf.Variable(tf.zeros([2]))\n",
    "yhat = tf.nn.softmax(tf.matmul(x_, W) + b)\n",
    "\n",
    "# Define our loss function\n",
    "mse_loss = tf.reduce_mean(tf.square(yhat - y_))\n",
    "\n",
    "# Compute accuracy computation\n",
    "correct_prediction = tf.equal(tf.argmax(yhat,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# Set up the training method\n",
    "train_step = tf.train.AdamOptimizer(0.1).minimize(mse_loss)\n",
    "isess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [1, 0]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(vectors, \n",
    "                                                     target, \n",
    "                                                     test_size=0.2, \n",
    "                                                     random_state=42)\n",
    "\n",
    "ytr = np.vstack(((y_train), (1-y_train))).T\n",
    "yte = np.vstack(((y_test), (1-y_test))).T\n",
    "ytr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4875"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy.eval(feed_dict={x_: X_train, y_: ytr})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True, False,  True, False, False,  True,  True,\n",
       "       False, False, False,  True, False, False, False,  True, False,\n",
       "       False, False,  True, False,  True, False,  True,  True,  True,\n",
       "        True, False, False,  True,  True,  True,  True, False,  True,\n",
       "       False,  True,  True, False, False, False, False, False, False,\n",
       "        True, False,  True,  True, False,  True, False, False,  True,\n",
       "        True, False, False, False,  True,  True, False,  True, False,\n",
       "        True,  True, False,  True, False,  True,  True,  True, False,\n",
       "       False, False,  True,  True, False, False, False,  True])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_prediction.eval(feed_dict={x_: X_train, y_: ytr})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train step: 0, Loss: 0.30002403259277344, Accuracy: 40.00000059604645\n",
      "Train step: 100, Loss: 0.13190464675426483, Accuracy: 80.0000011920929\n",
      "Train step: 200, Loss: 0.11605868488550186, Accuracy: 75.0\n",
      "Train step: 300, Loss: 0.11584951728582382, Accuracy: 75.0\n",
      "Train step: 400, Loss: 0.12045522034168243, Accuracy: 75.0\n",
      "Train step: 500, Loss: 0.12665247917175293, Accuracy: 80.0000011920929\n",
      "Train step: 600, Loss: 0.13285115361213684, Accuracy: 80.0000011920929\n",
      "Train step: 700, Loss: 0.1384759247303009, Accuracy: 80.0000011920929\n",
      "Train step: 800, Loss: 0.1434277594089508, Accuracy: 80.0000011920929\n",
      "Train step: 900, Loss: 0.14775830507278442, Accuracy: 69.9999988079071\n",
      "Train step: 1000, Loss: 0.15155087411403656, Accuracy: 69.9999988079071\n",
      "Train step: 1100, Loss: 0.15491712093353271, Accuracy: 69.9999988079071\n",
      "Train step: 1200, Loss: 0.15805566310882568, Accuracy: 69.9999988079071\n",
      "Train step: 1300, Loss: 0.16127391159534454, Accuracy: 69.9999988079071\n",
      "Train step: 1400, Loss: 0.16487812995910645, Accuracy: 69.9999988079071\n",
      "Train step: 1500, Loss: 0.16901203989982605, Accuracy: 69.9999988079071\n"
     ]
    }
   ],
   "source": [
    "# Train the model!\n",
    "n_iters = 1500\n",
    "for i in range(n_iters+1):\n",
    "    # Run through an iteration of the training process\n",
    "    train_step.run(feed_dict={x_: X_train, y_: ytr})\n",
    "    \n",
    "    # Compute the accuracy and loss\n",
    "    if i % 100 == 0:\n",
    "        current_loss = mse_loss.eval(feed_dict={x_: X_test, y_: yte})\n",
    "        current_acc  = accuracy.eval(feed_dict={x_: X_test, y_: yte})\n",
    "        print('Train step: {}, Loss: {}, Accuracy: {}'.format(i, \n",
    "                                                                current_loss, \n",
    "                                                                current_acc * 100.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/isachsquintana/Documents/pymy/twenty_newsgroups/v36/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py:118: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Train step: 0, Loss: 0.569, Accuracy: 40.000%\n",
      "Train step: 100, Loss: 0.258, Accuracy: 60.000%\n",
      "Train step: 200, Loss: 0.274, Accuracy: 60.000%\n",
      "Train step: 300, Loss: 0.251, Accuracy: 60.000%\n",
      "Train step: 400, Loss: 0.242, Accuracy: 60.000%\n",
      "Train step: 500, Loss: 0.239, Accuracy: 60.000%\n",
      "Train step: 600, Loss: 0.238, Accuracy: 60.000%\n",
      "Train step: 700, Loss: 0.239, Accuracy: 60.000%\n",
      "Train step: 800, Loss: 0.240, Accuracy: 60.000%\n",
      "Train step: 900, Loss: 0.241, Accuracy: 60.000%\n",
      "Train step: 1000, Loss: 0.241, Accuracy: 60.000%\n",
      "Train step: 1100, Loss: 0.242, Accuracy: 65.000%\n",
      "Train step: 1200, Loss: 0.242, Accuracy: 65.000%\n",
      "Train step: 1300, Loss: 0.242, Accuracy: 65.000%\n",
      "Train step: 1400, Loss: 0.241, Accuracy: 75.000%\n",
      "Train step: 1500, Loss: 0.241, Accuracy: 75.000%\n",
      "Train step: 1600, Loss: 0.240, Accuracy: 75.000%\n",
      "Train step: 1700, Loss: 0.238, Accuracy: 75.000%\n",
      "Train step: 1800, Loss: 0.237, Accuracy: 70.000%\n",
      "Train step: 1900, Loss: 0.236, Accuracy: 70.000%\n",
      "Train step: 2000, Loss: 0.235, Accuracy: 70.000%\n",
      "Train step: 2100, Loss: 0.234, Accuracy: 70.000%\n",
      "Train step: 2200, Loss: 0.233, Accuracy: 70.000%\n",
      "Train step: 2300, Loss: 0.232, Accuracy: 70.000%\n",
      "Train step: 2400, Loss: 0.231, Accuracy: 70.000%\n",
      "Train step: 2500, Loss: 0.229, Accuracy: 70.000%\n",
      "Train step: 2600, Loss: 0.228, Accuracy: 65.000%\n",
      "Train step: 2700, Loss: 0.227, Accuracy: 65.000%\n",
      "Train step: 2800, Loss: 0.226, Accuracy: 65.000%\n",
      "Train step: 2900, Loss: 0.224, Accuracy: 65.000%\n",
      "Train step: 3000, Loss: 0.223, Accuracy: 70.000%\n",
      "Train step: 3100, Loss: 0.222, Accuracy: 70.000%\n",
      "Train step: 3200, Loss: 0.220, Accuracy: 75.000%\n",
      "Train step: 3300, Loss: 0.219, Accuracy: 75.000%\n",
      "Train step: 3400, Loss: 0.218, Accuracy: 75.000%\n",
      "Train step: 3500, Loss: 0.216, Accuracy: 75.000%\n",
      "Train step: 3600, Loss: 0.215, Accuracy: 85.000%\n",
      "Train step: 3700, Loss: 0.214, Accuracy: 85.000%\n",
      "Train step: 3800, Loss: 0.213, Accuracy: 85.000%\n",
      "Train step: 3900, Loss: 0.211, Accuracy: 85.000%\n",
      "Train step: 4000, Loss: 0.210, Accuracy: 85.000%\n",
      "Train step: 4100, Loss: 0.209, Accuracy: 80.000%\n",
      "Train step: 4200, Loss: 0.208, Accuracy: 80.000%\n",
      "Train step: 4300, Loss: 0.207, Accuracy: 80.000%\n",
      "Train step: 4400, Loss: 0.206, Accuracy: 80.000%\n",
      "Train step: 4500, Loss: 0.205, Accuracy: 80.000%\n",
      "Train step: 4600, Loss: 0.204, Accuracy: 80.000%\n",
      "Train step: 4700, Loss: 0.203, Accuracy: 80.000%\n",
      "Train step: 4800, Loss: 0.202, Accuracy: 80.000%\n",
      "Train step: 4900, Loss: 0.201, Accuracy: 80.000%\n",
      "Train step: 5000, Loss: 0.200, Accuracy: 80.000%\n",
      "Train step: 5100, Loss: 0.199, Accuracy: 80.000%\n",
      "Train step: 5200, Loss: 0.198, Accuracy: 80.000%\n",
      "Train step: 5300, Loss: 0.197, Accuracy: 80.000%\n",
      "Train step: 5400, Loss: 0.195, Accuracy: 80.000%\n",
      "Train step: 5500, Loss: 0.194, Accuracy: 80.000%\n",
      "Train step: 5600, Loss: 0.193, Accuracy: 80.000%\n",
      "Train step: 5700, Loss: 0.192, Accuracy: 80.000%\n",
      "Train step: 5800, Loss: 0.190, Accuracy: 80.000%\n",
      "Train step: 5900, Loss: 0.189, Accuracy: 80.000%\n",
      "Train step: 6000, Loss: 0.188, Accuracy: 80.000%\n",
      "Train step: 6100, Loss: 0.187, Accuracy: 80.000%\n",
      "Train step: 6200, Loss: 0.185, Accuracy: 80.000%\n",
      "Train step: 6300, Loss: 0.184, Accuracy: 80.000%\n",
      "Train step: 6400, Loss: 0.183, Accuracy: 80.000%\n",
      "Train step: 6500, Loss: 0.182, Accuracy: 80.000%\n",
      "Train step: 6600, Loss: 0.181, Accuracy: 80.000%\n",
      "Train step: 6700, Loss: 0.180, Accuracy: 80.000%\n",
      "Train step: 6800, Loss: 0.178, Accuracy: 80.000%\n",
      "Train step: 6900, Loss: 0.177, Accuracy: 80.000%\n",
      "Train step: 7000, Loss: 0.176, Accuracy: 80.000%\n",
      "Train step: 7100, Loss: 0.175, Accuracy: 80.000%\n",
      "Train step: 7200, Loss: 0.174, Accuracy: 80.000%\n",
      "Train step: 7300, Loss: 0.173, Accuracy: 80.000%\n",
      "Train step: 7400, Loss: 0.172, Accuracy: 80.000%\n",
      "Train step: 7500, Loss: 0.171, Accuracy: 80.000%\n",
      "Train step: 7600, Loss: 0.170, Accuracy: 80.000%\n",
      "Train step: 7700, Loss: 0.169, Accuracy: 80.000%\n",
      "Train step: 7800, Loss: 0.167, Accuracy: 80.000%\n",
      "Train step: 7900, Loss: 0.166, Accuracy: 80.000%\n",
      "Train step: 8000, Loss: 0.165, Accuracy: 85.000%\n",
      "Train step: 8100, Loss: 0.164, Accuracy: 85.000%\n",
      "Train step: 8200, Loss: 0.163, Accuracy: 85.000%\n",
      "Train step: 8300, Loss: 0.162, Accuracy: 85.000%\n",
      "Train step: 8400, Loss: 0.161, Accuracy: 85.000%\n",
      "Train step: 8500, Loss: 0.160, Accuracy: 85.000%\n",
      "Train step: 8600, Loss: 0.159, Accuracy: 85.000%\n",
      "Train step: 8700, Loss: 0.158, Accuracy: 85.000%\n",
      "Train step: 8800, Loss: 0.157, Accuracy: 85.000%\n",
      "Train step: 8900, Loss: 0.156, Accuracy: 85.000%\n",
      "Train step: 9000, Loss: 0.155, Accuracy: 85.000%\n",
      "Train step: 9100, Loss: 0.154, Accuracy: 85.000%\n",
      "Train step: 9200, Loss: 0.153, Accuracy: 85.000%\n",
      "Train step: 9300, Loss: 0.152, Accuracy: 85.000%\n",
      "Train step: 9400, Loss: 0.151, Accuracy: 85.000%\n",
      "Train step: 9500, Loss: 0.151, Accuracy: 85.000%\n",
      "Train step: 9600, Loss: 0.150, Accuracy: 85.000%\n",
      "Train step: 9700, Loss: 0.149, Accuracy: 85.000%\n",
      "Train step: 9800, Loss: 0.148, Accuracy: 80.000%\n",
      "Train step: 9900, Loss: 0.147, Accuracy: 80.000%\n",
      "Train step: 10000, Loss: 0.146, Accuracy: 80.000%\n",
      "Train step: 10100, Loss: 0.145, Accuracy: 80.000%\n",
      "Train step: 10200, Loss: 0.144, Accuracy: 80.000%\n",
      "Train step: 10300, Loss: 0.144, Accuracy: 80.000%\n",
      "Train step: 10400, Loss: 0.143, Accuracy: 80.000%\n",
      "Train step: 10500, Loss: 0.142, Accuracy: 80.000%\n",
      "Train step: 10600, Loss: 0.141, Accuracy: 80.000%\n",
      "Train step: 10700, Loss: 0.140, Accuracy: 80.000%\n",
      "Train step: 10800, Loss: 0.140, Accuracy: 80.000%\n",
      "Train step: 10900, Loss: 0.139, Accuracy: 80.000%\n",
      "Train step: 11000, Loss: 0.138, Accuracy: 80.000%\n",
      "Train step: 11100, Loss: 0.137, Accuracy: 80.000%\n",
      "Train step: 11200, Loss: 0.137, Accuracy: 80.000%\n",
      "Train step: 11300, Loss: 0.136, Accuracy: 80.000%\n",
      "Train step: 11400, Loss: 0.135, Accuracy: 80.000%\n",
      "Train step: 11500, Loss: 0.135, Accuracy: 80.000%\n",
      "Train step: 11600, Loss: 0.134, Accuracy: 80.000%\n",
      "Train step: 11700, Loss: 0.133, Accuracy: 80.000%\n",
      "Train step: 11800, Loss: 0.133, Accuracy: 80.000%\n",
      "Train step: 11900, Loss: 0.132, Accuracy: 80.000%\n",
      "Train step: 12000, Loss: 0.131, Accuracy: 80.000%\n",
      "Train step: 12100, Loss: 0.131, Accuracy: 80.000%\n",
      "Train step: 12200, Loss: 0.130, Accuracy: 80.000%\n",
      "Train step: 12300, Loss: 0.130, Accuracy: 80.000%\n",
      "Train step: 12400, Loss: 0.129, Accuracy: 80.000%\n",
      "Train step: 12500, Loss: 0.128, Accuracy: 80.000%\n",
      "Train step: 12600, Loss: 0.128, Accuracy: 80.000%\n",
      "Train step: 12700, Loss: 0.127, Accuracy: 80.000%\n",
      "Train step: 12800, Loss: 0.127, Accuracy: 80.000%\n",
      "Train step: 12900, Loss: 0.126, Accuracy: 80.000%\n",
      "Train step: 13000, Loss: 0.126, Accuracy: 80.000%\n",
      "Train step: 13100, Loss: 0.125, Accuracy: 80.000%\n",
      "Train step: 13200, Loss: 0.125, Accuracy: 80.000%\n",
      "Train step: 13300, Loss: 0.124, Accuracy: 80.000%\n",
      "Train step: 13400, Loss: 0.124, Accuracy: 80.000%\n",
      "Train step: 13500, Loss: 0.124, Accuracy: 80.000%\n",
      "Train step: 13600, Loss: 0.123, Accuracy: 80.000%\n",
      "Train step: 13700, Loss: 0.123, Accuracy: 80.000%\n",
      "Train step: 13800, Loss: 0.122, Accuracy: 80.000%\n",
      "Train step: 13900, Loss: 0.122, Accuracy: 80.000%\n",
      "Train step: 14000, Loss: 0.122, Accuracy: 80.000%\n",
      "Train step: 14100, Loss: 0.121, Accuracy: 80.000%\n",
      "Train step: 14200, Loss: 0.121, Accuracy: 80.000%\n",
      "Train step: 14300, Loss: 0.121, Accuracy: 80.000%\n",
      "Train step: 14400, Loss: 0.120, Accuracy: 80.000%\n",
      "Train step: 14500, Loss: 0.120, Accuracy: 80.000%\n",
      "Train step: 14600, Loss: 0.120, Accuracy: 80.000%\n",
      "Train step: 14700, Loss: 0.119, Accuracy: 80.000%\n",
      "Train step: 14800, Loss: 0.119, Accuracy: 80.000%\n",
      "Train step: 14900, Loss: 0.119, Accuracy: 80.000%\n",
      "Train step: 15000, Loss: 0.118, Accuracy: 80.000%\n"
     ]
    }
   ],
   "source": [
    "# Now we are creating two weight matrices, one that contains the\n",
    "# weights connecting the input units to the hidden units, and one\n",
    "# connecting the hidden units to the output units\n",
    "n_inputs = 300\n",
    "n_hidden = 300\n",
    "n_outputs = 2\n",
    "W_input_to_hidden = tf.Variable(tf.truncated_normal([n_inputs, n_hidden], stddev=0.1))\n",
    "W_hidden_to_output = tf.Variable(tf.truncated_normal([n_hidden, n_outputs], stddev=0.1))\n",
    "b_hidden = tf.Variable(tf.constant(0.1, shape=[n_hidden]))\n",
    "b_output = tf.Variable(tf.constant(0.1, shape=[n_outputs]))\n",
    "\n",
    "# We now redefine the neural computation. I'm showing it here in\n",
    "# two steps: one for each layer in the network\n",
    "hidden_activation = tf.nn.sigmoid(tf.matmul(x_, W_input_to_hidden) + b_hidden)\n",
    "yhat = tf.nn.softmax(tf.matmul(hidden_activation, W_hidden_to_output) + b_output)\n",
    "\n",
    "############################\n",
    "# The rest is the same...\n",
    "mse_loss = tf.reduce_mean(tf.square(yhat - y_))\n",
    "correct_prediction = tf.equal(tf.argmax(yhat,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "train_step = tf.train.AdagradOptimizer(0.1).minimize(mse_loss)\n",
    "isess.run(tf.initialize_all_variables())\n",
    "n_iters = 15000\n",
    "for i in range(n_iters+1):\n",
    "    train_step.run(feed_dict={x_: X_train, y_: ytr})\n",
    "    if i % 100 == 0:\n",
    "        current_loss = mse_loss.eval(feed_dict={x_: X_test, y_: yte})\n",
    "        current_acc  = accuracy.eval(feed_dict={x_: X_test, y_: yte})\n",
    "        print('Train step: %d, Loss: %.3f, Accuracy: %.3f%%' % (i, \n",
    "                                                                current_loss, \n",
    "                                                                current_acc * 100.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
